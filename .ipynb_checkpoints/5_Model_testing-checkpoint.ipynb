{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4be8ae6",
   "metadata": {},
   "source": [
    "# Tests of trained models\n",
    "\n",
    "I will use this notebook to create a function, to test the models that are being generated during the development of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66f655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for \"gym\" and \"retro\" features\n",
    "from gym import Env\n",
    "import gym\n",
    "from gym.spaces import MultiDiscrete, Box, MultiBinary, Discrete\n",
    "from gym.wrappers import Monitor as gymMon\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import retro\n",
    "from retro import RetroEnv\n",
    "\n",
    "# To help with image preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# To help with the operating system\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Everything concerning SB3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbd322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetroMtpoNesReducedRL(Env):\n",
    "    \"\"\"\n",
    "    Class that creates a retro \"Gym\" object, and allows me to manipulate its observation space.\n",
    "     With this I seek to reduce the observations space, to speed up the training stage.\n",
    "\n",
    "     This class creates a \"focus area\", removing the outter most two thirds of the screen (vertically), \n",
    "     leaving in \"focus\" a area where the action of the game takes place. Additionally I reduce the number\n",
    "     of color channels, from three to one, which gives the feeling that the game is in black and white (also\n",
    "     called \"grayscaling\").\n",
    "\n",
    "     In this class, additionally, the \"viewing\" area is reduced, going from an observation space of 196x80x1\n",
    "     to one of 84x84x1.\n",
    "     \n",
    "     The main inspiration for this class comes from a Youtube tutorial from Nickolas Renotte.\n",
    "     \n",
    "     https://www.youtube.com/watch?v=rzbFhu6So5U&t=6248s\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, state='GlassJoe.state',\n",
    "                 scenario='scenario_king_hippo',\n",
    "                 inttype=retro.data.Integrations.STABLE,\n",
    "                 points_as_rewards=True):\n",
    "        super(RetroEnv).__init__()\n",
    "        # Most of these lines comes from GYM RETRO library.\n",
    "        self.img = None\n",
    "        rom_path = retro.data.get_romfile_path('Mtpo-Nes', inttype)\n",
    "        self.system = retro.get_romfile_system(rom_path)\n",
    "        core = retro.get_system_info(self.system)\n",
    "        self.buttons = core['buttons']\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(9)\n",
    "        self.state = state\n",
    "        self.scenario = scenario\n",
    "        self.game = retro.make(game='Mtpo-Nes',\n",
    "                               state=self.state,\n",
    "                               scenario=self.scenario,\n",
    "                              )\n",
    "        self.points_as_rewards = points_as_rewards\n",
    "        self.picture = None\n",
    "        \n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\" \n",
    "        Method to preprocess the images that the \"RetroEnv\" object uses during training.\n",
    "         The idea is to deliver a reduced observation, which helps streamline the training processes of the\n",
    "         agent. The derivation of the reduced observation can be seen in the notebook:\n",
    "        \n",
    "         - '1_CV_Preprocessing.ipynb'\n",
    "        \n",
    "         which is part of this 'Notebooks' section\n",
    "        \"\"\"\n",
    "        # Cropping\n",
    "        xlen = observation.shape[0]\n",
    "        ylen = observation.shape[1]\n",
    "        focus_zone = observation[int(xlen*(1/8)):int(xlen*(3/2)),int(ylen/3):-int(ylen/3)]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(focus_zone, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # We must fit the output to a tensor with three dimensions, since\n",
    "        # it is the data structure that the gym object expects.\n",
    "        # values between 0 and 1.\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def reset(self):\n",
    "        # Returns the fist \"frame\"\n",
    "        obs = self.game.reset()\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        self.score = 0\n",
    "        self.picture = processed_obs\n",
    "        return processed_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Go one step further in the emulation of the game\n",
    "        # Integrate the modification to the observation using the \"preprocessed()\" method\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        \n",
    "        # This is to return the points of the game as the reward if we want it.\n",
    "        if self.points_as_rewards:\n",
    "            reward_as_points = info['POINTS'] - self.score\n",
    "            self.score = info['POINTS']\n",
    "            return processed_obs, reward_as_points, done, info\n",
    "        else:  \n",
    "            return processed_obs, reward, done, info\n",
    "    \n",
    "    # The rest of the methods are not used much, yet might come in\n",
    "    # handy in some cases\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.picture\n",
    "    \n",
    "    def get_buttons(self):\n",
    "        return self.buttons\n",
    "    \n",
    "    def get_action_meaning(self, act):\n",
    "        return self.game.get_action_meaning(act)\n",
    "    \n",
    "    def get_in_game_score(self):\n",
    "        return self.score\n",
    "\n",
    "    def get_in_game_reward(self):\n",
    "        return self.in_game_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2449855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetroMtpoNesReduced(Env):\n",
    "    \"\"\"\n",
    "    Clase que crea un objeto retro \"gym\", y me permite manipular el espacio de observaciones del mismo.\n",
    "    Con esto busco reducir el espacio de observaciones, para acelerar la etapa de entrenamiento.\n",
    "\n",
    "    Esta clase crea un \"area de foco\", eliminando dos tercios de la pantalla (verticalmente), más especificamente\n",
    "    eliminando dos franjas verticales de los extremos, dejando en \"foco\" el are donde se lleva a cabo la acción\n",
    "    del juego. Adicionalmente reduzco la cantidad de canales de color, de tres a uno, dejando solo un canal,\n",
    "    lo que da la sensación de que el juego es en blanco y negro.\n",
    "\n",
    "    En esta clase, adicionalmente, se realiza una reducción del area de visión, pasando de un espacio de observaciones de 196x80x1\n",
    "    a uno de 84x84x1, y permitiendo que el agente solo \"observe\" la diferencia entre el \"frame\" actual, y el anterior, \n",
    "    y no todo el \"frame\" completo. \n",
    "    \"\"\"\n",
    "    def __init__(self, state='GlassJoe.state', scenario='scenario.json', inttype=retro.data.Integrations.STABLE,\n",
    "        points_as_rewards=True):\n",
    "        super(RetroEnv).__init__()\n",
    "        self.img = None\n",
    "        rom_path = retro.data.get_romfile_path('Mtpo-Nes', inttype)\n",
    "        self.system = retro.get_romfile_system(rom_path)\n",
    "        core = retro.get_system_info(self.system)\n",
    "        self.buttons = core['buttons']\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "#         self.action_space = MultiBinary(12)\n",
    "        self.action_space = Discrete(24)\n",
    "        self.state = state\n",
    "        self.scenario = scenario\n",
    "        self.game = retro.make(game='Mtpo-Nes', state=self.state, scenario=self.scenario)\n",
    "        self.points_as_rewards = points_as_rewards\n",
    "        self.picture = None\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\" Metodo para preprocesar las imagenes que el objeto \"env\" utiliza durante el entrenamiento.\n",
    "        La idea es entregar una observación reducida, que ayude a agilizar los procesos de entrenamiento del\n",
    "        agente. La derivación de la observación reducida puede verse en el notebook: \n",
    "        \n",
    "        - \"2_aletelecom_CV_Preprocessing.ipynb\"\n",
    "        \n",
    "        que es parte de esta sección de \"Notebooks\"\n",
    "        \"\"\"\n",
    "        # Cropping\n",
    "        xlen = observation.shape[0]\n",
    "        ylen = observation.shape[1]\n",
    "        focus_zone = observation[int(xlen*(1/8)):int(xlen*(3/2)),int(ylen/3):-int(ylen/3)]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(focus_zone, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Debemos ajustar la salida a un tensor con tres dimensiones, debido a que\n",
    "        # es la estructura de datos que espra el objeto gym.\n",
    "        # También dividimos la salida de canales entre \"255\" para \"normalizar\"\n",
    "        # los valores entre 0 y 1.\n",
    "        channels = np.reshape(resize / 255, (84,84,1))\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def reset(self):\n",
    "        # Retorna el primer \"frame\"\n",
    "        # Sin cambios a la implementación original\n",
    "        obs = self.game.reset()\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        self.score = 0\n",
    "        self.previous_frame = processed_obs\n",
    "        frame_delta = processed_obs - self.previous_frame\n",
    "        self.picture = frame_delta\n",
    "        return processed_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Avanza un paso en la emulación del juego\n",
    "        # Integra la modificación a la observación mediante el metodo \"preprocessed()\"\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        processed_obs = self.preprocess(obs)\n",
    "\n",
    "        # La variable \"frame_delta\" es la diferencia entre el \"frame\" anterior\n",
    "        # el actual, lo que \"muestra\" al agente solo las diferencias entre\n",
    "        # cada cuadro, reduciendo aún más el procesamiento.\n",
    "        frame_delta = processed_obs - self.previous_frame\n",
    "        self.previous_frame = processed_obs   \n",
    "        self.picture = frame_delta\n",
    "        if self.points_as_rewards:\n",
    "            reward_as_points = info['POINTS'] - self.score\n",
    "            self.score = info['POINTS']\n",
    "            return frame_delta, reward_as_points, done, info\n",
    "        else:  \n",
    "            return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d508e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MtpoDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        USE_STAR = [\n",
    "        [], # Sin movimiento\n",
    "        ['RIGHT'], # Esquiva a la derecha\n",
    "        ['LEFT'], # Esquiva a la izquierda\n",
    "        ['DOWN'], # Se cubre\n",
    "        ['UP', 'A'], # Golpea a la cara con un derechazo\n",
    "        ['UP', 'B'], # Golpea a la cara con un izquierdazo\n",
    "        ['A'], # Golpea al cuerpo con un derechazo\n",
    "        ['B'], # Golpea al cuerpo con un izquierdazo\n",
    "        ['START'], # Utiliza súper poder\n",
    "        ]\n",
    "        \n",
    "        DODGE = [\n",
    "        [], # Sin movimiento\n",
    "        ['RIGHT'], # Esquiva a la derecha\n",
    "        ['LEFT'], # Esquiva a la izquierda\n",
    "        ['UP', 'A'], # Golpea a la cara con un derechazo\n",
    "        ['UP', 'B'], # Golpea a la cara con un izquierdazo\n",
    "        ['A'], # Golpea al cuerpo con un derechazo\n",
    "        ['B'], # Golpea al cuerpo con un izquierdazo\n",
    "        ['START'], # Utiliza súper poder\n",
    "        ]\n",
    "\n",
    "# Acciones para no utilizar la estrella durante la pelea (súper poder)\n",
    "        NO_STAR = [\n",
    "        [], # Sin movimiento\n",
    "        ['RIGHT'], # Esquiva a la derecha\n",
    "        ['LEFT'], # Esquiva a la izquierda\n",
    "        ['DOWN'], # Se cubre\n",
    "        ['UP', 'A'], # Golpea a la cara con un derechazo\n",
    "        ['UP', 'B'], # Golpea a la cara con un izquierdazo\n",
    "        ['A'], # Golpea al cuerpo con un derechazo\n",
    "        ['B'], # Golpea al cuerpo con un izquierdazo\n",
    "        ]\n",
    "        \n",
    "        super().__init__(env=env, combos=DODGE)\n",
    "        \n",
    "        def get_action(self):\n",
    "            print(self.buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533d79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetroMtpoNesReducedGail(Env):\n",
    "    \"\"\"\n",
    "    Clase que crea un objeto retro \"gym\", y me permite manipular el espacio de observaciones del mismo.\n",
    "    Con esto busco reducir el espacio de observaciones, para acelerar la etapa de entrenamiento.\n",
    "\n",
    "    Esta clase crea un \"area de foco\", eliminando dos tercios de la pantalla (verticalmente), más especificamente\n",
    "    eliminando dos franjas verticales de los extremos, dejando en \"foco\" el are donde se lleva a cabo la acción\n",
    "    del juego. Adicionalmente reduzco la cantidad de canales de color, de tres a uno, dejando solo un canal,\n",
    "    lo que da la sensación de que el juego es en blanco y negro.\n",
    "\n",
    "    En esta clase, adicionalmente, se realiza una reducción del area de visión, pasando de un espacio de observaciones de 196x80x1\n",
    "    a uno de 84x84x1, y permitiendo que el agente solo \"observe\" la diferencia entre el \"frame\" actual, y el anterior, \n",
    "    y no todo el \"frame\" completo. \n",
    "    \"\"\"\n",
    "    def __init__(self, state='GlassJoe.state',\n",
    "                 scenario='scenario.json',\n",
    "                 inttype=retro.data.Integrations.STABLE,\n",
    "                 use_restricted_actions=retro.Actions.DISCRETE,\n",
    "                 points_as_rewards=True):\n",
    "        super(RetroEnv).__init__()\n",
    "        self.img = None\n",
    "        rom_path = retro.data.get_romfile_path('Mtpo-Nes', inttype)\n",
    "        self.system = retro.get_romfile_system(rom_path)\n",
    "        core = retro.get_system_info(self.system)\n",
    "        self.buttons = core['buttons']\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "#         self.action_space = Discrete(32)\n",
    "#         self.observation_space = Box(low=0, high=1, shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(24)\n",
    "        self.state = state\n",
    "        self.scenario = scenario\n",
    "        self.use_restricted_actions = use_restricted_actions\n",
    "        self.game = retro.make(game='Mtpo-Nes',\n",
    "                               state=self.state,\n",
    "                               scenario=self.scenario,\n",
    "                               use_restricted_actions=self.use_restricted_actions)\n",
    "        self.points_as_rewards = points_as_rewards\n",
    "        self.picture = None\n",
    "        \n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\" Metodo para preprocesar las imagenes que el objeto \"env\" utiliza durante el entrenamiento.\n",
    "        La idea es entregar una observación reducida, que ayude a agilizar los procesos de entrenamiento del\n",
    "        agente. La derivación de la observación reducida puede verse en el notebook: \n",
    "        \n",
    "        - \"2_aletelecom_CV_Preprocessing.ipynb\"\n",
    "        \n",
    "        que es parte de esta sección de \"Notebooks\"\n",
    "        \"\"\"\n",
    "        # Cropping\n",
    "        xlen = observation.shape[0]\n",
    "        ylen = observation.shape[1]\n",
    "        focus_zone = observation[int(xlen*(1/8)):int(xlen*(3/2)),int(ylen/3):-int(ylen/3)]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(focus_zone, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Debemos ajustar la salida a un tensor con tres dimensiones, debido a que\n",
    "        # es la estructura de datos que espra el objeto gym.\n",
    "        # También dividimos la salida de canales entre \"255\" para \"normalizar\"\n",
    "        # los valores entre 0 y 1.\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def reset(self):\n",
    "        # Retorna el primer \"frame\"\n",
    "        # Sin cambios a la implementación original\n",
    "        obs = self.game.reset()\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        self.score = 0\n",
    "        self.previous_frame = processed_obs\n",
    "        frame_delta = processed_obs - self.previous_frame\n",
    "        self.picture = frame_delta\n",
    "        return processed_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Avanza un paso en la emulación del juego\n",
    "        # Integra la modificación a la observación mediante el metodo \"preprocessed()\"\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        \n",
    "        # La variable \"frame_delta\" es la diferencia entre el \"frame\" anterior\n",
    "        # el actual, lo que \"muestra\" al agente solo las diferencias entre\n",
    "        # cada cuadro, reduciendo aún más el procesamiento.\n",
    "        frame_delta = processed_obs - self.previous_frame\n",
    "        self.previous_frame = processed_obs   \n",
    "        self.picture = frame_delta\n",
    "        if self.points_as_rewards:\n",
    "            reward_as_points = info['POINTS'] - self.score\n",
    "            self.score = info['POINTS']\n",
    "            return frame_delta, reward_as_points, done, info\n",
    "        else:  \n",
    "            return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.picture\n",
    "    \n",
    "    def get_buttons(self):\n",
    "        return self.buttons\n",
    "    \n",
    "    def get_action_meaning(self, act):\n",
    "        return self.game.get_action_meaning(act)\n",
    "    \n",
    "    def get_in_game_score(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7847b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_STATE = 'KingHippo.state'\n",
    "# GAME_STATE = 'GlassJoe.state'\n",
    "SCENARIO = \"scenario_score\"\n",
    "#SCENARIO = 'scenario_king_hippo'\n",
    "# CLASS = 'FUCKED'\n",
    "# CLASS = 'REDUCED'\n",
    "CLASS = 'GAIL'\n",
    "STACKED_FRAMES = 12\n",
    "POINTS_AS_REWARDS = True\n",
    "\n",
    "VIDEO_RECORD_PATH = os.path.join('..', 'video_gifs')\n",
    "if CLASS == 'REDUCED':\n",
    "    env = RetroMtpoNesReduced(\n",
    "        state=GAME_STATE,\n",
    "        scenario=SCENARIO,\n",
    "#         video_record_path=VIDEO_RECORD_PATH\n",
    "    )\n",
    "#     env = MtpoDiscretizer(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, STACKED_FRAMES, channels_order='last')\n",
    "elif CLASS == 'FUCKED':\n",
    "    env = RetroMtpoNes(\n",
    "        state=GAME_STATE,\n",
    "        scenario=SCENARIO,\n",
    "       video_record_path=VIDEO_RECORD_PATH\n",
    "    )\n",
    "#     env = MtpoDiscretizer(env)\n",
    "#     env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, STACKED_FRAMES, channels_order='last')\n",
    "elif CLASS == 'GAIL':\n",
    "    env = RetroMtpoNesReducedGail(\n",
    "        state=GAME_STATE,\n",
    "        scenario=SCENARIO,\n",
    "        use_restricted_actions=retro.Actions.DISCRETE,\n",
    "        points_as_rewards=POINTS_AS_REWARDS\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "#     env = VecTransposeImage(env)\n",
    "    env = VecFrameStack(env, STACKED_FRAMES, channels_order='last')\n",
    "    env = VecTransposeImage(env, skip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed5503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366ada65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_objects = {\n",
    "# #     \"lr_schedule\": lambda x: .003,#x,\n",
    "#     \"clip_range\": lambda x: 0.02#x\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_objects = {\n",
    "#             \"learning_rate\": lambda _: 0.0,\n",
    "#             \"lr_schedule\": lambda _: 0.0,\n",
    "#             \"clip_range\": lambda _: 0.0,\n",
    "#             \"gamma\": lambda _: 0.0,\n",
    "#             \"ent_coef\": lambda _: 0.0\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67c3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = 'PPO_VK_checkpoint_model_2000000.zip'\n",
    "# MODEL_NAME = 'trial_VK_0_best_model.zip'\n",
    "# #MODEL_PATH = os.path.join('..', 'models', 'train', 'vonkaiser', MODEL_NAME)\n",
    "# MODEL_PATH = os.path.join('..', 'models', 'opt', 'vonkaiser_2', MODEL_NAME)\n",
    "# #MODEL_PATH = os.path.join(MODEL_NAME)\n",
    "# model = PPO.load(MODEL_PATH, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9fc2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'PPO_model_750000S_12SF_DISCRETIZED_PUNCHES.zip'\n",
    "# MODEL_NAME = 'PPO_008592522717275543_model_2000000S_12SF_DISCRETIZED_POINTS.zip'\n",
    "\n",
    "MODEL_NAME = 'Partial_GAIL-MLP(1024)-Default-PPO_MLP_HWC_model_550_rounds_54TrajExp.zip'\n",
    "\n",
    "\n",
    "# MODEL_PATH = os.path.join('..', 'models', 'train', 'glassjoe', MODEL_NAME)\n",
    "# MODEL_PATH = os.path.join('..', 'notebooks', MODEL_NAME)\n",
    "MODEL_PATH = os.path.join('..', 'models', 'train', 'gail', 'kinghippo', MODEL_NAME)\n",
    "\n",
    "model = PPO.load(MODEL_PATH)\n",
    "\n",
    "# model = PPO.load(MODEL_PATH, print_system_info=True, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b154ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imitation.policies.serialize import load_policy\n",
    "# from imitation.util import util\n",
    "\n",
    "# POLICY_NAME = 'PPO-DAGGER-MLP(1024)-1000-policy_PRETRAINED_KingHippo_POINTS.zip'\n",
    "# POLICY_PATH = os.path.join('..', 'models', 'train', 'dagger', 'kinghippo', POLICY_NAME)\n",
    "# local_policy = load_policy(\"ppo\", env, loader_kwargs={\"path\": POLICY_PATH})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb9beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0bdbd5",
   "metadata": {},
   "source": [
    "Observamos el modelo jugar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30fae6",
   "metadata": {},
   "source": [
    "# El mejor modelo hasta ahora es el 5 PPO_47 para Glass Joe\n",
    "\n",
    "# El mejor modelo hasta ahora es el 09 PPO_10 para Von Kaiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74bd7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [13:48<00:00,  8.29s/it]\n"
     ]
    }
   ],
   "source": [
    "VELOCIDAD = 0.001\n",
    "GAMES = 100\n",
    "\n",
    "total_reward = np.zeros(shape=(GAMES,))\n",
    "for n in tqdm(range(GAMES)):\n",
    "    episode_rew = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done: \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        action, _ = model.predict(obs)\n",
    "#         action = [env.action_space.sample()]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rew += reward\n",
    "#         env.render()\n",
    "#         time.sleep(VELOCIDAD)\n",
    "    total_reward[n] = float(episode_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dc426f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb64433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_EVAL_EPISODES = 100\n",
    "\n",
    "# mean_reward, std_reward = evaluate_policy(model,\n",
    "#                 env,\n",
    "#                 n_eval_episodes=N_EVAL_EPISODES,\n",
    "#                 return_episode_rewards=True,\n",
    "#                 deterministic=False,\n",
    "#                 render=True,\n",
    "#                # reward_threshold=10.0\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92a266fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7b60d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(model,\n",
    "#                 env,\n",
    "#                 n_eval_episodes=N_EVAL_EPISODES,\n",
    "#                 return_episode_rewards=True,\n",
    "#                 deterministic=True,\n",
    "#                 render=True,\n",
    "#                # reward_threshold=10.0\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15483cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43386c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664409ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_RETRO",
   "language": "python",
   "name": "rl_retro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
