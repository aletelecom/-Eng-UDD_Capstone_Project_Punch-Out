{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3962d04",
   "metadata": {},
   "source": [
    "# Exploration of vectorization and discretization of the action space\n",
    "\n",
    "In the SB3 documentation, the vectorization of the __\"Env\"__ objects is recommended, in order to reduce the processing time necessary for the different stages of a project.\n",
    "\n",
    "As a starting point we are going to use the \"RetroMtpoNesReduced()\" class, and we are going to expand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654b21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# For gym functionality\n",
    "from gym import Env\n",
    "import gym\n",
    "from retro import RetroEnv \n",
    "from gym.spaces import MultiDiscrete, Box, Discrete, MultiBinary\n",
    "import retro\n",
    "import retro.data\n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# To use the stable baselines 3 objects and methods\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797309f6",
   "metadata": {},
   "source": [
    "## Utilización de Wrappers\n",
    "\n",
    "Las librerías SB3 y Retro, introducen la opción de utilizar objetos de programación llamados \"wrappers\", que permiten la modificación del comportamiento de los objetos \"env\", de manera de facilitar nuestro trabajo, y permitirnos libertad a la hora de construir nustros proyectos. Es por lo anterior que vamos a utilizar algunos \"wrappers\" que proveen dichas librerías, y adicionalmente vamos a modificar la clase \"MtpoNes()\", por una clase __\"RetroMtpoNes()\"__ que nos va a permitir utilizar un \"wrapper\" que nos auyde con la reducción del espacio de acciones de nuestro agente.\n",
    "\n",
    "La clase RetroMtpoNes() será la que utilizaremos definitivamente durante la ejecución del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetroMtpoNesReducedRL(Env):\n",
    "    \"\"\"\n",
    "    Class that creates a retro \"Gym\" object, and allows me to manipulate its observation space.\n",
    "     With this I seek to reduce the observations space, to speed up the training stage.\n",
    "\n",
    "     This class creates a \"focus area\", removing the outter most two thirds of the screen (vertically), \n",
    "     leaving in \"focus\" a area where the action of the game takes place. Additionally I reduce the number\n",
    "     of color channels, from three to one, which gives the feeling that the game is in black and white (also\n",
    "     called \"grayscaling\").\n",
    "\n",
    "     In this class, additionally, the \"viewing\" area is reduced, going from an observation space of 196x80x1\n",
    "     to one of 84x84x1.\n",
    "     \n",
    "     The main inspiration for this class comes from a Youtube tutorial from Nickolas Renotte.\n",
    "     \n",
    "     https://www.youtube.com/watch?v=rzbFhu6So5U&t=6248s\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, state='GlassJoe.state',\n",
    "                 scenario='scenario_king_hippo',\n",
    "                 inttype=retro.data.Integrations.STABLE,\n",
    "                 points_as_rewards=True):\n",
    "        super(RetroEnv).__init__()\n",
    "        # Most of these lines comes from GYM RETRO library.\n",
    "        self.img = None\n",
    "        rom_path = retro.data.get_romfile_path('Mtpo-Nes', inttype)\n",
    "        self.system = retro.get_romfile_system(rom_path)\n",
    "        core = retro.get_system_info(self.system)\n",
    "        self.buttons = core['buttons']\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(9)\n",
    "        self.state = state\n",
    "        self.scenario = scenario\n",
    "        self.game = retro.make(game='Mtpo-Nes',\n",
    "                               state=self.state,\n",
    "                               scenario=self.scenario,\n",
    "                              )\n",
    "        self.points_as_rewards = points_as_rewards\n",
    "        self.picture = None\n",
    "        \n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\" \n",
    "        Method to preprocess the images that the \"RetroEnv\" object uses during training.\n",
    "         The idea is to deliver a reduced observation, which helps streamline the training processes of the\n",
    "         agent. The derivation of the reduced observation can be seen in the notebook:\n",
    "        \n",
    "         - '1_CV_Preprocessing.ipynb'\n",
    "        \n",
    "         which is part of this 'Notebooks' section\n",
    "        \"\"\"\n",
    "        # Cropping\n",
    "        xlen = observation.shape[0]\n",
    "        ylen = observation.shape[1]\n",
    "        focus_zone = observation[int(xlen*(1/8)):int(xlen*(3/2)),int(ylen/3):-int(ylen/3)]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(focus_zone, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # We must fit the output to a tensor with three dimensions, since\n",
    "        # it is the data structure that the gym object expects.\n",
    "        # values between 0 and 1.\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def reset(self):\n",
    "        # Returns the fist \"frame\"\n",
    "        obs = self.game.reset()\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        self.score = 0\n",
    "        self.picture = processed_obs\n",
    "        return processed_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Go one step further in the emulation of the game\n",
    "        # Integrate the modification to the observation using the \"preprocessed()\" method\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        \n",
    "        # This is to return the points of the game as the reward if we want it.\n",
    "        if self.points_as_rewards:\n",
    "            reward_as_points = info['POINTS'] - self.score\n",
    "            self.score = info['POINTS']\n",
    "            return processed_obs, reward_as_points, done, info\n",
    "        else:  \n",
    "            return processed_obs, reward, done, info\n",
    "    \n",
    "    # The rest of the methods are not used much, yet might come in\n",
    "    # handy in some cases\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.picture\n",
    "    \n",
    "    def get_buttons(self):\n",
    "        return self.buttons\n",
    "    \n",
    "    def get_action_meaning(self, act):\n",
    "        return self.game.get_action_meaning(act)\n",
    "    \n",
    "    def get_in_game_score(self):\n",
    "        return self.score\n",
    "\n",
    "    def get_in_game_reward(self):\n",
    "        return self.in_game_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d627cde",
   "metadata": {},
   "source": [
    "## Action space discretization wrapper\n",
    "\n",
    "As we mentioned, in order to reduce the action space of the RL agent, which helps us reduce the data processing time during training, we are going to implement a \"wrapper\" to our \"RetroMtpoNesReduced()\" object that help us with that.\n",
    "\n",
    "This \"wrapper\" is an example that we can find in the retro library repository:\n",
    "\n",
    "https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "\n",
    "I'll take it and adapt it to reduce the action space of __\"Punch-Out\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2498c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps an \"Env\" object and turn it into an environment with discrete actions.\n",
    "     args:\n",
    "         combos: ordered list of lists of valid button combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MtpoDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    We use discrete actions specific to the Punch-Out game\n",
    "    \"\"\"\n",
    "\n",
    "# Actions to use the star during the fight (super power)\n",
    "    def __init__(self, env):\n",
    "        USE_STAR = [\n",
    "        [], # Motionless\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ['START'], # Use super power\n",
    "        ]\n",
    "\n",
    "# Actions to not use the star during the fight (super power)\n",
    "        NO_STAR = [\n",
    "        [], # Motionless\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ]\n",
    "\n",
    "# Actions to not use the star during the fight (super power) and only dodge blows, not cover\n",
    "        DODGE = [\n",
    "        [],\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ['START'], # Use super power\n",
    "        ]\n",
    "        super().__init__(env=env, combos=DODGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d4f02",
   "metadata": {},
   "source": [
    "Now we initialize the \"env\" object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aaa10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RetroMtpoNesReducedRL()\n",
    "env = MtpoDiscretizer(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868e072",
   "metadata": {},
   "source": [
    "And to test that the \"wrapper\" discretization of the action space works, we are going to use a function provided by the SB3 library, called __\"env_checker.check_env\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c45be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62aacd5",
   "metadata": {},
   "source": [
    "If we don't get errors, that means that the \"RetroMtpoNesReducedRL()\" class and the \"wrapper\" work correctly.\n",
    "\n",
    "Now, we verify the new format of the agent's action space, and a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5478940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(9)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "obs = env.observation_space\n",
    "acciones = env.action_space\n",
    "print(acciones)\n",
    "print(acciones.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2ddb3",
   "metadata": {},
   "source": [
    "We see that now the __action space__ of the agent is discrete, and has only nine (09) posible actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_RETRO",
   "language": "python",
   "name": "rl_retro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
