{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371c9905",
   "metadata": {},
   "source": [
    "# Exploration of \"Callbacks\" and \"Hyperparameter optimization\"\n",
    "\n",
    "For the \"callbacks\" stage, and \"hyperparameter optimization\" (HPO), I am going to be guided by what is exposed in the \"colab\" tutorial proposed on the SB3 page:\n",
    "\n",
    "https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/4_callbacks_hyperparameter_tuning.ipynb\n",
    "\n",
    "As a training environment I will continue with the __\"RetroMtpoNesReducedRL()\"__ class, the action wrapper, and additionally I will add some additional \"wrappers\" that we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b0d5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for \"gym\" and \"retro\" features\n",
    "import os\n",
    "from gym import Env\n",
    "import gym\n",
    "from gym.spaces import MultiDiscrete, Box, MultiBinary\n",
    "import retro\n",
    "from retro import RetroEnv\n",
    "import time\n",
    "\n",
    "# To help with image preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Todo lo concerniente a SB3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# Para la optimización de hiperparametros\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e81f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetroMtpoNesReducedRL(Env):\n",
    "    \"\"\"\n",
    "    Class that creates a retro \"Gym\" object, and allows me to manipulate its observation space.\n",
    "     With this I seek to reduce the observations space, to speed up the training stage.\n",
    "\n",
    "     This class creates a \"focus area\", removing the outter most two thirds of the screen (vertically), \n",
    "     leaving in \"focus\" a area where the action of the game takes place. Additionally I reduce the number\n",
    "     of color channels, from three to one, which gives the feeling that the game is in black and white (also\n",
    "     called \"grayscaling\").\n",
    "\n",
    "     In this class, additionally, the \"viewing\" area is reduced, going from an observation space of 196x80x1\n",
    "     to one of 84x84x1.\n",
    "     \n",
    "     The main inspiration for this class comes from a Youtube tutorial from Nickolas Renotte.\n",
    "     \n",
    "     https://www.youtube.com/watch?v=rzbFhu6So5U&t=6248s\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, state='GlassJoe.state',\n",
    "                 scenario='scenario_king_hippo',\n",
    "                 inttype=retro.data.Integrations.STABLE,\n",
    "                 points_as_rewards=True):\n",
    "        super(RetroEnv).__init__()\n",
    "        # Most of these lines comes from GYM RETRO library.\n",
    "        self.img = None\n",
    "        rom_path = retro.data.get_romfile_path('Mtpo-Nes', inttype)\n",
    "        self.system = retro.get_romfile_system(rom_path)\n",
    "        core = retro.get_system_info(self.system)\n",
    "        self.buttons = core['buttons']\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(9)\n",
    "        self.state = state\n",
    "        self.scenario = scenario\n",
    "        self.game = retro.make(game='Mtpo-Nes',\n",
    "                               state=self.state,\n",
    "                               scenario=self.scenario,\n",
    "                              )\n",
    "        self.points_as_rewards = points_as_rewards\n",
    "        self.picture = None\n",
    "        \n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\" \n",
    "        Method to preprocess the images that the \"RetroEnv\" object uses during training.\n",
    "         The idea is to deliver a reduced observation, which helps streamline the training processes of the\n",
    "         agent. The derivation of the reduced observation can be seen in the notebook:\n",
    "        \n",
    "         - '1_CV_Preprocessing.ipynb'\n",
    "        \n",
    "         which is part of this 'Notebooks' section\n",
    "        \"\"\"\n",
    "        # Cropping\n",
    "        xlen = observation.shape[0]\n",
    "        ylen = observation.shape[1]\n",
    "        focus_zone = observation[int(xlen*(1/8)):int(xlen*(3/2)),int(ylen/3):-int(ylen/3)]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(focus_zone, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # We must fit the output to a tensor with three dimensions, since\n",
    "        # it is the data structure that the gym object expects.\n",
    "        # values between 0 and 1.\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def reset(self):\n",
    "        # Returns the fist \"frame\"\n",
    "        obs = self.game.reset()\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        self.score = 0\n",
    "        self.picture = processed_obs\n",
    "        return processed_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Go one step further in the emulation of the game\n",
    "        # Integrate the modification to the observation using the \"preprocessed()\" method\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        processed_obs = self.preprocess(obs)\n",
    "        \n",
    "        # This is to return the points of the game as the reward if we want it.\n",
    "        if self.points_as_rewards:\n",
    "            reward_as_points = info['POINTS'] - self.score\n",
    "            self.score = info['POINTS']\n",
    "            return processed_obs, reward_as_points, done, info\n",
    "        else:  \n",
    "            return processed_obs, reward, done, info\n",
    "    \n",
    "    # The rest of the methods are not used much, yet might come in\n",
    "    # handy in some cases\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.picture\n",
    "    \n",
    "    def get_buttons(self):\n",
    "        return self.buttons\n",
    "    \n",
    "    def get_action_meaning(self, act):\n",
    "        return self.game.get_action_meaning(act)\n",
    "    \n",
    "    def get_in_game_score(self):\n",
    "        return self.score\n",
    "\n",
    "    def get_in_game_reward(self):\n",
    "        return self.in_game_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0324968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps an \"Env\" object and turn it into an environment with discrete actions.\n",
    "     args:\n",
    "         combos: ordered list of lists of valid button combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class MtpoDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    We use discrete actions specific to the Punch-Out game\n",
    "    \"\"\"\n",
    "\n",
    "# Actions to use the star during the fight (super power)\n",
    "    def __init__(self, env):\n",
    "        USE_STAR = [\n",
    "        [], # Motionless\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ['START'], # Use super power\n",
    "        ]\n",
    "\n",
    "# Actions to not use the star during the fight (super power)\n",
    "        NO_STAR = [\n",
    "        [], # Motionless\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ]\n",
    "\n",
    "# Actions to not use the star during the fight (super power) and only dodge blows, not cover\n",
    "        DODGE = [\n",
    "        [],\n",
    "        ['RIGHT'], # Dodge right\n",
    "        ['LEFT'], # Dodge left\n",
    "        ['DOWN'], # Cover\n",
    "        ['UP', 'A'], # Hit the face with a right hand\n",
    "        ['UP', 'B'], # Hit the face with a left hand\n",
    "        ['A'], # Punch to the body with a right hand\n",
    "        ['B'], # Punch to the body with a left hand\n",
    "        ['START'], # Use super power\n",
    "        ]\n",
    "        super().__init__(env=env, combos=DODGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec898da1",
   "metadata": {},
   "source": [
    "## Enter Optuna\n",
    "\n",
    "To perform the hyperparameter optimization stage, we are going to use the __\"Optuna\"__ library, as suggested by the SB3 tutorials page, and the tutorial by youtuber Nickolas Renotte.\n",
    "\n",
    "The official page of Optuna is:\n",
    "\n",
    "https://optuna.org/\n",
    "\n",
    "__Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019.\n",
    "Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.__\n",
    "\n",
    "And the commented Youtube tutorial is:\n",
    "\n",
    "https://www.youtube.com/watch?v=rzbFhu6So5U&t=6224s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3842d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join('..\\models', 'logs', 'kinghippo')\n",
    "OPT_DIR = os.path.join('..\\models', 'opt', 'kinghippo')\n",
    "MODELS_PATH = os.path.join('..\\models', 'kinghippo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3906e64",
   "metadata": {},
   "source": [
    "To carry out the hyperparameter optimization stage, we are going to define a function that returns a dictionary, whose keys are the names of the parameters that we want to optimize, and their values are the __\"suggestions\"__ that I want to make to the system, all according to the __Optuna__ documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61028ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_float('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 3e-5, 3e-2),\n",
    "        'clip_range':trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbe028",
   "metadata": {},
   "source": [
    "Now we define a function that optimizes the parameters by running a loop that performs a short training, and then evaluates the model, this loop is called study, or __\"experiment\"__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa98385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward\n",
    "\n",
    "# Read recommendations of these parameters below\n",
    "TOTAL_TIME_STEPS = 2000\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "LEVEL_STATE = 'KingHippo.state'\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial) \n",
    "\n",
    "        # Create environment \n",
    "        env = RetroMtpoNesReducedRL(state=LEVEL_STATE)\n",
    "        env = MtpoDiscretizer(env)\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 12, channels_order='last')\n",
    "\n",
    "        # Create algo\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        # The total timesteps and eval episodes variables should be set accordingly:\n",
    "        # At least 50.000 for total timesteps, and\n",
    "        # at least 10 evaluation episodes\n",
    "        model.learn(total_timesteps=TOTAL_TIME_STEPS)\n",
    "\n",
    "        # Evaluate model\n",
    "        # Same as before here, with the evaluation variable\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=N_EVAL_EPISODES)\n",
    "        \n",
    "        # Change the name of the best model accordingly to the level you're training it.\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_KH_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0419212",
   "metadata": {},
   "source": [
    "Como paso final, definimos la dirección de optimización (por defecto es minimizar) que sea __\"maximizar\"__ para que el agente siempre busque obtener una recompenza mayor con cada intento, y definimos el número de intentos a que sean 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "298cae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-21 18:02:32,059]\u001b[0m A new study created in memory with name: no-name-e33b5128-9ff9-4cb4-a5e8-f613b0962b5c\u001b[0m\n",
      "C:\\Users\\armedina\\Documents\\DS-Projects\\python_envs\\rl_retro\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7184`, after every 112 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7184 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-21 18:06:37,420]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'n_steps': 7184, 'gamma': 0.8980419105445048, 'learning_rate': 0.013743209385947532, 'clip_range': 0.355394173701559, 'gae_lambda': 0.889373365703623}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "C:\\Users\\armedina\\Documents\\DS-Projects\\python_envs\\rl_retro\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3995`, after every 62 untruncated mini-batches, there will be a truncated mini-batch of size 27\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3995 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-21 18:08:44,289]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'n_steps': 3995, 'gamma': 0.9587801305324833, 'learning_rate': 0.02946527349611525, 'clip_range': 0.28095540738158253, 'gae_lambda': 0.8491697036789049}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "C:\\Users\\armedina\\Documents\\DS-Projects\\python_envs\\rl_retro\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6456`, after every 100 untruncated mini-batches, there will be a truncated mini-batch of size 56\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6456 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-21 18:12:14,080]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'n_steps': 6456, 'gamma': 0.9919438231659007, 'learning_rate': 0.027268484129795267, 'clip_range': 0.2360720136490084, 'gae_lambda': 0.9149281010477595}. Best is trial 0 with value: 0.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Creamos el experimento\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# As previously mentioned, here, the number of trials, should be over 100.\n",
    "study.optimize(optimize_agent, n_trials=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffb558",
   "metadata": {},
   "source": [
    "At the end of the optimization we obtain a set of suggested parameters, which we are going to assign to the model that will be trained, and additionally we are going to use the model obtained from the best \"experiment\" as a basis for training the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29ca42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of steps should be a multiple of 64, so we change it accordingly:\n",
    "\n",
    "study.best_params\n",
    "study.best_params['n_steps'] = (study.best_params['n_steps']//64) * study.best_params['n_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f302238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7184,\n",
       " 'gamma': 0.8980419105445048,\n",
       " 'learning_rate': 0.013743209385947532,\n",
       " 'clip_range': 0.355394173701559,\n",
       " 'gae_lambda': 0.889373365703623}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "727bcd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[0.0], datetime_start=datetime.datetime(2022, 12, 21, 18, 2, 32, 62171), datetime_complete=datetime.datetime(2022, 12, 21, 18, 6, 37, 419004), params={'n_steps': 7184, 'gamma': 0.8980419105445048, 'learning_rate': 0.013743209385947532, 'clip_range': 0.355394173701559, 'gae_lambda': 0.889373365703623}, distributions={'n_steps': IntDistribution(high=8192, log=False, low=2048, step=1), 'gamma': FloatDistribution(high=0.9999, log=False, low=0.8, step=None), 'learning_rate': FloatDistribution(high=0.03, log=False, low=3e-05, step=None), 'clip_range': FloatDistribution(high=0.4, log=False, low=0.1, step=None), 'gae_lambda': FloatDistribution(high=0.99, log=False, low=0.8, step=None)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b13f78",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "The \"callbacks\" are __\"warnings\"__, and __\"modifications\"__ that we can configure during the training phase, and that help us, among other things, to:\n",
    "\n",
    "* Modify the behavior of the training loop.\n",
    "* Modify the behavior of the parameters during training.\n",
    "* Save intermediate models during the training process.\n",
    "* Stop training if certain conditions that we impose are met.\n",
    "* Create, fill, and save a record file, or log, with which we can actively evaluate the training of the model in real time.\n",
    "\n",
    "## Tensorboard\n",
    "\n",
    "It is a tool created by Google as part of the Tensorflow library for the creation of machine learning models that allows recording the behavior in real time of the training of said models. When used together with the \"callbacks\" they are a powerful tool to know the development of the training of a model, allowing us to know visually if the training phase is going according to expectations, or if on the contrary it is deviating from that goal, which would help us cancel said training, among many other things.\n",
    "\n",
    "Next we will create a class called __\"TrainAndLoggingCallback()\"__ that inherits from the callbacks base class, and that will allow us to obtain all the features mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5194e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'PPO_checkpoint_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461ae80",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Using the parameters obtained through the \"HPO\" phase with Optuna, we are going to train the RL model using a PPO agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78db8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37c455d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL_STATE = 'KingHippo.state'\n",
    "\n",
    "env = RetroMtpoNesReducedRL(state=LEVEL_STATE)\n",
    "env = MtpoDiscretizer(env)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecTransposeImage(env)\n",
    "env = VecFrameStack(env, 12, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f3413b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_params_optuna = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100def90",
   "metadata": {},
   "source": [
    "We are going to save an intermediate model every 500 thousand frames (in the real project, this frequency should be a lot higher, around 500k or 1M steps), to periodically evaluate the behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a583e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = os.path.join('..', 'models', 'train', 'kinghippo')\n",
    "CHECK_FREQ = 10000\n",
    "callback = TrainAndLoggingCallback(check_freq=CHECK_FREQ, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c9469",
   "metadata": {},
   "source": [
    "The PPO agenet will be trained using a CNN policy (which works best for visual observations, like the screen of our \"env\" object), and the parameters of the better Optuna experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3596388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **best_model_params_optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffceba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run this cell only if we have a previously saved best model\n",
    "\n",
    "# MODEL_NAME = 'trial_VK_10_best_model.zip'\n",
    "# MODEL_PATH = os.path.join('..', 'models', 'opt', 'kinghippo', MODEL_NAME)\n",
    "# model.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aad388",
   "metadata": {},
   "source": [
    "Let's train the model for 3 million frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5df09f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ..\\models\\logs\\kinghippo\\PPO_12\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.63e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 97       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 7184     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.79e+03    |\n",
      "|    ep_rew_mean          | 2           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1734        |\n",
      "|    total_timesteps      | 14368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042621728 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.355       |\n",
      "|    entropy_loss         | -2.15       |\n",
      "|    explained_variance   | -2.76       |\n",
      "|    learning_rate        | 0.0137      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    value_loss           | 5.58e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.52e+03    |\n",
      "|    ep_rew_mean          | 2.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3704        |\n",
      "|    total_timesteps      | 21552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047306456 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.355       |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0137      |\n",
      "|    loss                 | 4.35e-07    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 0.0384      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25abbc10670>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comenzamos a entrenar\n",
    "model.learn(total_timesteps=20000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea784b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with an appropiate name\n",
    "model.save('CnnPolicy-20k-HPO-ReducedActions-KingHippo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6aaff",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e0fb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3ec302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL_STATE = 'KingHippo.state'\n",
    "\n",
    "env = RetroMtpoNesReducedRL(state=LEVEL_STATE)\n",
    "env = MtpoDiscretizer(env)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecTransposeImage(env)\n",
    "env = VecFrameStack(env, 12, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d2fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\n",
    "    \"lr_schedule\": lambda x: .003,\n",
    "    \"clip_range\": lambda x: .02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "679e89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('CnnPolicy-20k-HPO-ReducedActions-KingHippo.zip',\n",
    "#                  custom_objects=custom_objects\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57771ba3",
   "metadata": {},
   "source": [
    "Now lets test the model in some episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86886673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9ddcd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:59<00:00, 39.81s/it]\n"
     ]
    }
   ],
   "source": [
    "VELOCIDAD = 0.0001\n",
    "GAMES = 3\n",
    "\n",
    "episodes_rewards = np.zeros(shape=(GAMES,))\n",
    "for n in tqdm(range(GAMES)):\n",
    "    episode_rew = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done: \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rew += reward\n",
    "        env.render()\n",
    "        time.sleep(VELOCIDAD)\n",
    "    episodes_rewards[n] = float(episode_rew)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c8fb1",
   "metadata": {},
   "source": [
    "Finally we print the rewards of each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c53b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.,  0., 10.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_RETRO",
   "language": "python",
   "name": "rl_retro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
